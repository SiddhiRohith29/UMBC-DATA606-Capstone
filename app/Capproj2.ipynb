{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9261270a",
   "metadata": {},
   "source": [
    "# AIRLINE PASSENGER SATISFACTION SYSTEM PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73a395",
   "metadata": {},
   "source": [
    "Data Source: https://www.kaggle.com/datasets/ahmedelsharkaw/airline-passenger-satisfaction?select=airline_passenger_satisfaction.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c97568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b524ff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\siddh\\\\Desktop\\\\capstoneproj\\\\airline_passenger_satisfaction\\\\airline2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msiddh\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcapstoneproj\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mairline_passenger_satisfaction\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mairline2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m df\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\siddh\\\\Desktop\\\\capstoneproj\\\\airline_passenger_satisfaction\\\\airline2.csv'"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\siddh\\Desktop\\capstoneproj\\airline_passenger_satisfaction\\airline2.csv')\n",
    "df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433fe601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c735a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = ['Gender', 'Customer Type', 'Type of Travel', 'Class','Time_Convenience','Ease of Online Booking',\n",
    "                      'Check-in Service', 'Online Boarding', 'Gate Location','On-board Service', 'Seat Comfort', 'Leg Room Service',\n",
    "                      'Cleanliness','Food and Drink', 'In-flight Service', 'In-flight Wifi Service','In-flight Entertainment', \n",
    "                      'Baggage Handling','Satisfaction']\n",
    "\n",
    "df[columns_to_convert] = df[columns_to_convert].astype('category')\n",
    "df['Departure Delay']=df['Departure Delay'].astype('float64')\n",
    "df['Age']=df['Age'].astype('float64')\n",
    "df['Arrival Delay']=df['Arrival Delay'].astype('float64')\n",
    "df['Flight Distance']=df['Flight Distance'].astype('float64')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccaff09-03d4-4381-ba0a-80a492ca0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.copy()\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2aeb23",
   "metadata": {},
   "source": [
    "## DATA MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f7c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4faac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Iterate over each column in the DataFrame\n",
    "for column in df.columns:\n",
    "    # Check if the column is of type 'category'\n",
    "    if df[column].dtype == 'category':\n",
    "        # Apply label encoding\n",
    "        df[column] = label_encoder.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8eaaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df.drop(columns=['Satisfaction'])\n",
    "y = df['Satisfaction'] \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize classifiers including the additional ones\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Train, predict, and evaluate each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "    \n",
    "    # Evaluate the model\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)  # Compute ROC AUC score\n",
    "    print(f\"Classification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"ROC AUC Score for {name}: {roc_auc}\")\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9c1283",
   "metadata": {},
   "source": [
    "__Top Performers:__ XGBoost and Random Forest stand out with the highest accuracy and ROC AUC scores, making them the best choices for scenarios requiring high reliability and precision.\n",
    "\n",
    "__Good Choice for Simplicity and Speed:__ Logistic Regression offers a simpler, less computationally intensive option with decent performance metrics.\n",
    "\n",
    "__Needs Improvement:__ KNN underperforms in comparison to other models, suggesting it may not be the best fit for this particular dataset without further tuning or preprocessing.\n",
    "\n",
    "__Versatile and Effective:__ Gradient Boosting offers a very effective alternative to the tree-based ensemble models, with performance metrics close to those of Random Forest and XGBoost.\n",
    "\n",
    "* For most applications where both precision and recall are critical, and especially where the ability to discriminate is paramount, either XGBoost or Random Forest would be recommended. For less complex models where interpretability is key, Logistic Regression might be suitable. Gradient Boosting strikes a balance between complexity and performance, making it a strong candidate for many predictive tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5954e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a dictionary to hold accuracy scores for training and testing sets\n",
    "accuracy_scores = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    # Predict on training set and evaluate\n",
    "    train_pred = clf.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, train_pred)\n",
    "    \n",
    "    # Predict on testing set and evaluate\n",
    "    test_pred = clf.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    # Save the accuracies in the dictionary\n",
    "    accuracy_scores[name] = {\n",
    "        'Training Accuracy': train_accuracy,\n",
    "        'Testing Accuracy': test_accuracy,\n",
    "        'Accuracy Gap': abs(train_accuracy - test_accuracy)\n",
    "    }\n",
    "\n",
    "# Print the accuracy scores and check for overfitting or underfitting\n",
    "for name, scores in accuracy_scores.items():\n",
    "    print(f\"{name} - Training Accuracy: {scores['Training Accuracy']:.2f}, Testing Accuracy: {scores['Testing Accuracy']:.2f}, Accuracy Gap: {scores['Accuracy Gap']:.2f}\")\n",
    "    if scores['Accuracy Gap'] > 0.1:\n",
    "        if scores['Testing Accuracy'] < 0.6:\n",
    "            print(f\"{name} may be underfitting severely.\")\n",
    "        else:\n",
    "            print(f\"{name} may be overfitting; consider regularization or simplifying the model.\")\n",
    "    elif scores['Testing Accuracy'] < 0.6:\n",
    "        print(f\"{name} seems to be underperforming; may need more training or model tuning.\")\n",
    "    else:\n",
    "        print(f\"{name} seems well-fitted.\")\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf59152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = {}\n",
    "for name, clf in classifiers.items():\n",
    "    scores = cross_val_score(clf, X, y, cv=10)\n",
    "    cv_scores[name] = scores\n",
    "    print(f\"{name} - Cross-Validation Accuracy: {scores.mean():.2f} (+/- {scores.std() * 2:.2f})\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96308b1",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "__Most in Need of Tuning:__ XGBoost and Gradient Boosting are the most parameter-sensitive models and can see significant gains from careful tuning.\n",
    "\n",
    "__Moderate Tuning:__ Random Forest, while often robust with default settings, also benefits significantly from tuning. Logistic Regression and KNN have fewer parameters but can still see performance improvements with fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Setup the grid search\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search_xgb.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search_xgb.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d720013",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search_xgb.best_estimator_\n",
    "satisfaction_predictions = best_model.predict(X_test)\n",
    "print(satisfaction_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc53f02-1694-4223-935f-e7b011639c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df1.drop('Satisfaction', axis=1)  # Drop the target to isolate features\n",
    "y = df1['Satisfaction']  # Target variable\n",
    "\n",
    "# Convert categorical variables into dummy/indicator variables\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the Random Forest Classifier\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance = sorted(zip(importances, X.columns), reverse=True)\n",
    "\n",
    "# Display the top 5 features\n",
    "print(\"Top 20 features:\")\n",
    "for importance, feature in feature_importance[:20]:\n",
    "    print(f\"{feature}: {importance}\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d705b06-10fd-4db1-ab39-d7b1762fb7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1[['Cleanliness','Food and Drink','Online Boarding','In-flight Wifi Service','Seat Comfort','Satisfaction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f1fea-743c-436e-bd54-07068b500fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Iterate over each column in the DataFrame\n",
    "for column in df1.columns:\n",
    "    # Check if the column is of type 'category'\n",
    "    if df1[column].dtype == 'category':\n",
    "        # Apply label encoding\n",
    "        df1[column] = label_encoder.fit_transform(df1[column])\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df1.drop(columns=['Satisfaction'])\n",
    "y = df1['Satisfaction'] \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize classifiers including the additional ones\n",
    "classifiers = {\n",
    "    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Train, predict, and evaluate each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "    \n",
    "    # Evaluate the model\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)  # Compute ROC AUC score\n",
    "    print(f\"Classification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"ROC AUC Score for {name}: {roc_auc}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785058d-1e20-4f14-97ff-5d4ee1f7019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Setup the grid search\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search_xgb.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search_xgb.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b11627-a0f5-49c9-97cf-1a1c70a8521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model1 = grid_search_xgb.best_estimator_\n",
    "satisfaction_predictions = best_model1.predict(X_test)\n",
    "print(satisfaction_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837516c4-ceb0-419e-85d5-d9180d4e2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "data = {\"model\": best_model1}\n",
    "# Collect inputs for the available attributes\n",
    "cleanliness = int(input(\"Rate Cleanliness (1-5): \"))\n",
    "food_and_drink = int(input(\"Rate Food and Drink (1-5): \"))\n",
    "online_boarding = int(input(\"Rate Online Boarding (1-5): \"))\n",
    "wifi_service = int(input(\"Rate In-flight Wifi Service (1-5): \"))\n",
    "seat_comfort = int(input(\"Rate Seat Comfort (1-5): \"))\n",
    "\n",
    "# Create a DataFrame with these values\n",
    "input_data = pd.DataFrame({\n",
    "    'Cleanliness': [cleanliness],\n",
    "    'Food and Drink': [food_and_drink],\n",
    "    'Online Boarding': [online_boarding],\n",
    "    'In-flight Wifi Service': [wifi_service],\n",
    "    'Seat Comfort': [seat_comfort]\n",
    "})\n",
    "\n",
    "# Ensure all data types are correct (here, all inputs are expected to be numeric so no additional conversion is needed)\n",
    "\n",
    "# Load the best model (make sure it's loaded if this is a new session or environment)\n",
    "with open('best_model1.pkl', 'wb') as file:\n",
    "    pickle.dump(data, file)\n",
    "    \n",
    "# Predict satisfaction (example, adjust based on actual output handling)\n",
    "prediction = best_model1.predict(input_data)\n",
    "prediction_label = \"Satisfied\" if prediction[0] == 1 else \"Dissatisfied\"\n",
    "print(\"Prediction:\", prediction_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224bf8f-5a73-4172-a7c4-17dc50906961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1712637181905,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
